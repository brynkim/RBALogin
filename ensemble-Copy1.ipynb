{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deepod.models.tabular import DevNet, PReNet, DeepSAD, FeaWAD, RoSAS\n",
    "from deepod.metrics import tabular_metrics\n",
    "from autoencodernn import *\n",
    "from tapnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 35/35 [00:11<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for filename in tqdm(os.listdir('./data/')):\n",
    "    if 'preprocessed' in filename:\n",
    "        dfs.append(pd.read_csv(f'./data/{filename}', index_col = 0))\n",
    "df = pd.concat(dfs).reset_index(drop = True)\n",
    "del(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_df, test_df = train_test_split(df, test_size = 0.2, stratify = df['label'])\n",
    "train_df, valid_df = train_test_split(train_valid_df, test_size = 0.125, stratify = train_valid_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_df.drop(['label'], axis = 1)\n",
    "# y_train = train_df['label']\n",
    "\n",
    "# X_valid = valid_df.drop(['label'], axis = 1)\n",
    "# y_valid = valid_df['label']\n",
    "\n",
    "# X_test = test_df.drop(['label'], axis = 1)\n",
    "# y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv('./checkpoint/x_train.csv')\n",
    "# y_train.to_csv('./checkpoint/y_train.csv')\n",
    "\n",
    "# X_valid.to_csv('./checkpoint/x_valid.csv')\n",
    "# y_valid.to_csv('./checkpoint/y_valid.csv')\n",
    "\n",
    "# X_test.to_csv('./checkpoint/x_test.csv')\n",
    "# y_test.to_csv('./checkpoint/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./checkpoint/x_train.csv')\n",
    "y_train = pd.read_csv('./checkpoint/y_train.csv')\n",
    "\n",
    "X_valid = pd.read_csv('./checkpoint/x_valid.csv')\n",
    "y_valid = pd.read_csv('./checkpoint/y_valid.csv')\n",
    "\n",
    "X_test = pd.read_csv('./checkpoint/x_test.csv')\n",
    "y_test = pd.read_csv('./checkpoint/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_preprocessing(df1, df2, df3):\n",
    "    df1['type'] = 'train'\n",
    "    df2['type'] = 'valid'\n",
    "    df3['type'] = 'test'\n",
    "\n",
    "    df = pd.concat([df1, df2, df3]).reset_index(drop = True)\n",
    "\n",
    "    country_onehot = pd.get_dummies(df['country']).astype(int)\n",
    "    risk_grades = df[['region_risk_grade', 'city_risk_grade', 'name_risk_grade']]\n",
    "    browser_onehot = pd.get_dummies(df['browser_name']).astype(int)\n",
    "    os_onehot = pd.get_dummies(df['os_name']).astype(int)\n",
    "    legacys = df[['browser_is_legacy', 'os_is_legacy']]\n",
    "    device_types = pd.get_dummies(df['device_type']).astype(int)\n",
    "    rtts = df['rtt']\n",
    "    type = df['type']\n",
    "    label = df['label']\n",
    "    df = pd.concat([country_onehot, risk_grades, browser_onehot, os_onehot, legacys, device_types, rtts, type, label], axis = 1)\n",
    "\n",
    "    df1 = df[df['type'] == 'train'].drop('type', axis = 1)\n",
    "    df2 = df[df['type'] == 'valid'].drop('type', axis = 1)\n",
    "    df3 = df[df['type'] == 'test'].drop('type', axis = 1)\n",
    "\n",
    "    return df1, df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Detection Model\n",
      "start - 2023-12-10 10:08:11.300161\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=220, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=1, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 2.497916, time: 101.3s\n",
      "epoch 10, training loss: 2.500566, time: 99.9s\n",
      "epoch 30, training loss: 2.500028, time: 99.5s\n",
      "epoch 40, training loss: 2.494651, time: 99.9s\n",
      "epoch 70, training loss: 2.491216, time: 100.0s\n",
      "epoch 80, training loss: 2.489513, time: 99.9s\n",
      "epoch 90, training loss: 2.489794, time: 99.3s\n",
      "epoch100, training loss: 2.490551, time: 99.3s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100% 16653/16653 [00:29<00:00, 568.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100% 16653/16653 [00:29<00:00, 573.96it/s]\n",
      "testing: 100% 2379/2379 [00:04<00:00, 581.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained with <deepod.models.tabular.devnet.DevNet object at 0x7fad9d16fc10>\n",
      "Train - AUC: 0.7447359488557014, AP: 0.32677451001144814, F1: 0.5345130108361602\n",
      "Valid - AUC: 0.7446193461758034, AP: 0.32537918443120517, F1: 0.5332294333741512\n",
      "end - 2023-12-10 12:53:31.695852 \n",
      "\n",
      "start - 2023-12-10 12:53:31.695873\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "training data counter: Counter({0: 971745, -1: 94036})\n",
      "MLPnet(\n",
      "  (network): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (linear): Linear(in_features=220, out_features=100, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (1): LinearBlock(\n",
      "      (linear): Linear(in_features=100, out_features=50, bias=False)\n",
      "      (act_layer): ReLU()\n",
      "    )\n",
      "    (2): LinearBlock(\n",
      "      (linear): Linear(in_features=50, out_features=128, bias=False)\n",
      "      (act_layer): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 30.813188, time: 92.3s\n",
      "epoch 10, training loss: 1.198376, time: 92.4s\n",
      "epoch 20, training loss: 1.027989, time: 94.2s\n",
      "epoch 30, training loss: 0.957365, time: 90.5s\n",
      "epoch 40, training loss: 0.861385, time: 92.9s\n",
      "epoch 50, training loss: 0.732488, time: 91.0s\n",
      "epoch 60, training loss: 0.780495, time: 92.2s\n",
      "epoch 80, training loss: 0.757915, time: 82.0s\n"
     ]
    }
   ],
   "source": [
    "# Anomaly Detection\n",
    "print('Anomaly Detection Model')\n",
    "ad_model_names = ['DevNet', 'PReNet', 'DeepSAD', 'FeaWAD', 'RoSAS']\n",
    "ad_models = [\n",
    "    DevNet(),           # 86.5s\n",
    "    # PReNet(),           # Too long. (2.4 hours)\n",
    "    DeepSAD(),          # 75.4s\n",
    "    # FeaWAD(epochs = 10000, lr = 0.01),           # Very fast but poor.\n",
    "    # RoSAS(),            # Too long.\n",
    "]\n",
    "\n",
    "train_df_ad, valid_df_ad, test_df_ad = anomaly_preprocessing(train_df, valid_df, test_df)\n",
    "\n",
    "X_train_ad = train_df_ad.drop(['label'], axis = 1)\n",
    "y_train_ad = train_df_ad['label']\n",
    "\n",
    "X_valid_ad = valid_df_ad.drop(['label'], axis = 1)\n",
    "y_valid_ad = valid_df_ad['label']\n",
    "\n",
    "X_test_ad = test_df_ad.drop(['label'], axis = 1)\n",
    "y_test_ad = test_df_ad['label']\n",
    "\n",
    "for model_name, model in zip(ad_model_names, ad_models):\n",
    "    print('start -', datetime.now())\n",
    "    \n",
    "    model.fit(X_train_ad.to_numpy(), y_train_ad.to_numpy())\n",
    "    print('Train Finish')\n",
    "    pred_train = (model.decision_function(X_train_ad.to_numpy()) > 0.5).astype(int)\n",
    "    auc_train, ap_train, f1_train = tabular_metrics(y_train_ad, pred_train)\n",
    "    \n",
    "    pred_valid = (model.decision_function(X_valid_ad.to_numpy()) > 0.5).astype(int)\n",
    "    auc_valid, ap_valid, f1_valid = tabular_metrics(y_valid_ad, pred_valid)\n",
    "    \n",
    "    print(f'Trained with {model}')\n",
    "    print(f'Train - AUC: {auc_train}, AP: {ap_train}, F1: {f1_train}')\n",
    "    print(f'Valid - AUC: {auc_valid}, AP: {ap_valid}, F1: {f1_valid}')\n",
    "\n",
    "    models.append(model)\n",
    "    print('end -', datetime.now(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder + NN\n",
    "print('AutoEncoder + NN Model')\n",
    "autoencoder_nn_model = model_v2(\n",
    "    train_data = train_df,          # Train set\n",
    "    valid_data = valid_df,          # Validation set\n",
    "    test_data = test_df,            # Test set\n",
    "    criteria = 0.5,                 # Classification threshold\n",
    "    split_ratio = [7, 1, 2],        # split ratio (format: [train,validation,test])\n",
    "    autoencoder_epochs = 50,        # epochs of autoencoder\n",
    "    classifier_epochs = 200,        # epochs of classifier\n",
    "    weight_for_attack = 15,         # weight for attack\n",
    ")\n",
    "\n",
    "models.append(autoencoder_nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNet\n",
    "print('TabNet Model')\n",
    "selected_columns = ['country_code', 'region', 'city_risk_grade', 'name_risk_grade', 'login_success', 'browser_is_legacy', 'os_is_legacy', 'rtt', 'device_type', 'label']\n",
    "categorical_columns = ['country_code', 'device_type', 'region']\n",
    "\n",
    "tabnet_model = TabNetModel(train_df, valid_df, test_df, selected_columns, categorical_columns, 'label')         #, pre_train_epochs = 5, epochs = 5)\n",
    "models.append(tabnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./checkpoint/models.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongjae_rba",
   "language": "python",
   "name": "dongjae_rba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd79beb0aae4abe819c785a6067139c3e0331b942f5d097b7bed8a75a3313abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
