{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepod.models.tabular import DevNet, PReNet, DeepSAD, FeaWAD, RoSAS\n",
    "from deepod.metrics import tabular_metrics\n",
    "from autoencodernn import *\n",
    "from tapnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 27/35 [00:03<00:00,  8.41it/s]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m tqdm(os\u001b[39m.\u001b[39mlistdir(\u001b[39m'\u001b[39m\u001b[39m./data/\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpreprocessed\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m filename:\n\u001b[0;32m----> 4\u001b[0m         dfs\u001b[39m.\u001b[39mappend(pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/\u001b[39;49m\u001b[39m{\u001b[39;49;00mfilename\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, index_col \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs)\u001b[39m.\u001b[39mreset_index(drop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[39mdel\u001b[39;00m(dfs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for filename in tqdm(os.listdir('./data/')):\n",
    "    if 'preprocessed' in filename:\n",
    "        dfs.append(pd.read_csv(f'./data/{filename}', index_col = 0))\n",
    "df = pd.concat(dfs).reset_index(drop = True)\n",
    "del(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_df, test_df = train_test_split(df, test_size = 0.2, stratify = df['label'])\n",
    "train_df, valid_df = train_test_split(train_valid_df, test_size = 0.125, stratify = train_valid_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['label'], axis = 1)\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_valid = valid_df.drop(['label'], axis = 1)\n",
    "y_valid = valid_df['label']\n",
    "\n",
    "X_test = test_df.drop(['label'], axis = 1)\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('./checkpoint/train_df.csv', index_col = 0)\n",
    "# valid_df = pd.read_csv('./checkpoint/valid_df.csv', index_col = 0)\n",
    "# test_df = pd.read_csv('./checkpoint/test_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.read_csv('./checkpoint/x_train.csv')\n",
    "# y_train = pd.read_csv('./checkpoint/y_train.csv')\n",
    "\n",
    "# X_valid = pd.read_csv('./checkpoint/x_valid.csv')\n",
    "# y_valid = pd.read_csv('./checkpoint/y_valid.csv')\n",
    "\n",
    "# X_test = pd.read_csv('./checkpoint/x_test.csv')\n",
    "# y_test = pd.read_csv('./checkpoint/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_preprocessing(df1, df2, df3):\n",
    "    df1['type'] = 'train'\n",
    "    df2['type'] = 'valid'\n",
    "    df3['type'] = 'test'\n",
    "\n",
    "    df = pd.concat([df1, df2, df3]).reset_index(drop = True)\n",
    "\n",
    "    country_onehot = pd.get_dummies(df['country']).astype(int)\n",
    "    risk_grades = df[['region_risk_grade', 'city_risk_grade', 'name_risk_grade']]\n",
    "    browser_onehot = pd.get_dummies(df['browser_name']).astype(int)\n",
    "    os_onehot = pd.get_dummies(df['os_name']).astype(int)\n",
    "    legacys = df[['browser_is_legacy', 'os_is_legacy']]\n",
    "    device_types = pd.get_dummies(df['device_type']).astype(int)\n",
    "    rtts = df['rtt']\n",
    "    type = df['type']\n",
    "    label = df['label']\n",
    "    df = pd.concat([country_onehot, risk_grades, browser_onehot, os_onehot, legacys, device_types, rtts, type, label], axis = 1)\n",
    "\n",
    "    df1 = df[df['type'] == 'train'].drop('type', axis = 1)\n",
    "    df2 = df[df['type'] == 'valid'].drop('type', axis = 1)\n",
    "    df3 = df[df['type'] == 'test'].drop('type', axis = 1)\n",
    "\n",
    "    return df1, df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection\n",
    "print('Anomaly Detection Model')\n",
    "ad_model_names = ['DevNet', 'PReNet', 'DeepSAD', 'FeaWAD', 'RoSAS']\n",
    "ad_models = [\n",
    "    # DevNet(),           # 86.5s\n",
    "    # PReNet(),           # Too long. (2.4 hours)\n",
    "    DeepSAD(epochs = 50),          # 75.4s\n",
    "    # FeaWAD(epochs = 10000, lr = 0.01),           # Very fast but poor.\n",
    "    # RoSAS(),            # Too long.\n",
    "]\n",
    "\n",
    "train_df_ad, valid_df_ad, test_df_ad = anomaly_preprocessing(train_df, valid_df, test_df)\n",
    "\n",
    "X_train_ad = train_df_ad.drop(['label'], axis = 1)\n",
    "y_train_ad = train_df_ad['label']\n",
    "\n",
    "X_valid_ad = valid_df_ad.drop(['label'], axis = 1)\n",
    "y_valid_ad = valid_df_ad['label']\n",
    "\n",
    "X_test_ad = test_df_ad.drop(['label'], axis = 1)\n",
    "y_test_ad = test_df_ad['label']\n",
    "\n",
    "for model_name, model in zip(ad_model_names, ad_models):\n",
    "    print('start -', datetime.now())\n",
    "    \n",
    "    model.fit(X_train_ad.to_numpy(), y_train_ad.to_numpy())\n",
    "    print('Train Finish')\n",
    "    pred_train = (model.decision_function(X_train_ad.to_numpy()) > 0.5).astype(int)\n",
    "    auc_train, ap_train, f1_train = tabular_metrics(y_train_ad, pred_train)\n",
    "    \n",
    "    pred_valid = (model.decision_function(X_valid_ad.to_numpy()) > 0.5).astype(int)\n",
    "    auc_valid, ap_valid, f1_valid = tabular_metrics(y_valid_ad, pred_valid)\n",
    "    \n",
    "    print(f'Trained with {model}')\n",
    "    print(f'Train - AUC: {auc_train}, AP: {ap_train}, F1: {f1_train}')\n",
    "    print(f'Valid - AUC: {auc_valid}, AP: {ap_valid}, F1: {f1_valid}')\n",
    "\n",
    "    models.append(model)\n",
    "    print('end -', datetime.now(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: For faster execution, use this cell and comment the upper one.\n",
    "# with open('./checkpoint/deepsad.pkl', 'rb') as f:\n",
    "#     deepsad_model = pickle.load(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder + NN\n",
    "print('AutoEncoder + NN Model')\n",
    "autoencoder_nn_model = model_v2(\n",
    "    train_data = train_df,          # Train set\n",
    "    valid_data = valid_df,          # Validation set\n",
    "    test_data = test_df,            # Test set\n",
    "    criteria = 0.5,                 # Classification threshold\n",
    "    split_ratio = [7, 1, 2],        # split ratio (format: [train,validation,test])\n",
    "    autoencoder_epochs = 50,        # epochs of autoencoder\n",
    "    classifier_epochs = 200,        # epochs of classifier\n",
    "    weight_for_attack = 15,         # weight for attack\n",
    ")\n",
    "\n",
    "models.append(autoencoder_nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: For faster execution, use this cell and comment the upper one.\n",
    "# with open('./checkpoint/autoencoder_nn.pkl', 'rb') as f:\n",
    "#     autoencoder_nn_model = pickle.load(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNet\n",
    "print('TabNet Model')\n",
    "selected_columns = ['country_code', 'region', 'city_risk_grade', 'name_risk_grade', 'login_success', 'browser_is_legacy', 'os_is_legacy', 'rtt', 'device_type', 'label']\n",
    "categorical_columns = ['country_code', 'device_type', 'region']\n",
    "\n",
    "tabnet_model = TabNetModel(train_df, valid_df, test_df, selected_columns, categorical_columns, 'label')         #, pre_train_epochs = 5, epochs = 5)\n",
    "models.append(tabnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: For faster execution, use this cell and comment the upper one.\n",
    "# with open('./checkpoint/tabnet.pkl', 'rb') as f:\n",
    "#     tabnet_model = pickle.load(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_pred = (deepsad_model.decision_function(X_test_ad.to_numpy()) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.to_numpy(), anomaly_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test.to_numpy(), anomaly_pred))\n",
    "print(precision_score(y_test.to_numpy(), anomaly_pred))\n",
    "print(recall_score(y_test.to_numpy(), anomaly_pred))\n",
    "print(f1_score(y_test.to_numpy(), anomaly_pred))\n",
    "print(geometric_mean_score(y_test.to_numpy(), anomaly_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_df = autoencoder_nn_model.predicted_df\n",
    "autonn_pred = an_df['Predicted Label'].to_numpy().astype(int)\n",
    "print(classification_report(an_df['Actual Label'], an_df['Predicted Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(an_df['Actual Label'].to_numpy(), autonn_pred))\n",
    "print(precision_score(an_df['Actual Label'].to_numpy(), autonn_pred))\n",
    "print(recall_score(an_df['Actual Label'].to_numpy(), autonn_pred))\n",
    "print(f1_score(an_df['Actual Label'].to_numpy(), autonn_pred))\n",
    "print(geometric_mean_score(an_df['Actual Label'].to_numpy(), autonn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_soft = tabnet_model.clf.predict_proba(tabnet_model.X_test.values)[:, 1]\n",
    "tabnet_pred = tabnet_soft > 0.5\n",
    "print(classification_report(tabnet_model.y_test, tabnet_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(tabnet_model.y_test, tabnet_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test.to_numpy(), tabnet_pred))\n",
    "print(precision_score(y_test.to_numpy(), tabnet_pred))\n",
    "print(recall_score(y_test.to_numpy(), tabnet_pred))\n",
    "print(f1_score(y_test.to_numpy(), tabnet_pred))\n",
    "print(geometric_mean_score(y_test.to_numpy(), tabnet_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp1 = ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), anomaly_pred, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'})\n",
    "disp2 = ConfusionMatrixDisplay.from_predictions(an_df['Actual Label'].astype(int), an_df['Predicted Label'].astype(int), normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'})\n",
    "disp3 = ConfusionMatrixDisplay.from_predictions(tabnet_model.y_test, tabnet_pred, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority Voting\n",
    "majority_pred = (np.stack([anomaly_pred, autonn_pred, tabnet_pred]).mean(axis = 0) > 0.5).astype(int)\n",
    "majority_disp = ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), majority_pred, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test.to_numpy(), majority_pred))\n",
    "print(precision_score(y_test.to_numpy(), majority_pred))\n",
    "print(recall_score(y_test.to_numpy(), majority_pred))\n",
    "print(f1_score(y_test.to_numpy(), majority_pred))\n",
    "print(geometric_mean_score(y_test.to_numpy(), majority_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or Voting\n",
    "or_pred = (np.stack([anomaly_pred, autonn_pred, tabnet_pred]).sum(axis = 0) != 0).astype(int)\n",
    "or_disp = ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), or_pred, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test.to_numpy(), or_pred))\n",
    "print(precision_score(y_test.to_numpy(), or_pred))\n",
    "print(recall_score(y_test.to_numpy(), or_pred))\n",
    "print(f1_score(y_test.to_numpy(), or_pred))\n",
    "print(geometric_mean_score(y_test.to_numpy(), or_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting\n",
    "deepsad_soft = deepsad_model.decision_function(X_test_ad.to_numpy())\n",
    "autonn_soft = an_df['Probability'].to_numpy()\n",
    "# tabnet_soft = tabnet_model.clf.predict_proba(tabnet_model.X_test.values)[:, 1]\n",
    "tabnet_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./checkpoint/deepsad_soft.pkl', 'wb') as f:\n",
    "    pickle.dump(deepsad_soft, f)\n",
    "with open('./checkpoint/autonn_soft.pkl', 'wb') as f:\n",
    "    pickle.dump(autonn_soft, f)\n",
    "with open('./checkpoint/tabnet_soft.pkl', 'wb') as f:\n",
    "    pickle.dump(tabnet_soft, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_pred = deepsad_soft.astype(float) + autonn_soft.astype(float) + tabnet_soft.astype(float) > 1.5\n",
    "soft_disp = ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), soft_pred, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test.to_numpy(), soft_pred))\n",
    "print(precision_score(y_test.to_numpy(), soft_pred))\n",
    "print(recall_score(y_test.to_numpy(), soft_pred))\n",
    "print(f1_score(y_test.to_numpy(), soft_pred))\n",
    "print(geometric_mean_score(y_test.to_numpy(), soft_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(y_true, y_pred, target_tpr):\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_pred)\n",
    "    index = np.argmin(np.abs(tpr - target_tpr))\n",
    "    return threshold[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsad_threshold_09990 = get_threshold(y_test.to_numpy(), deepsad_soft, 0.9990)\n",
    "deepsad_threshold_09950 = get_threshold(y_test.to_numpy(), deepsad_soft, 0.9950)\n",
    "deepsad_threshold_09900 = get_threshold(y_test.to_numpy(), deepsad_soft, 0.9900)\n",
    "deepsad_threshold_09800 = get_threshold(y_test.to_numpy(), deepsad_soft, 0.9800)\n",
    "deepsad_threshold_09700 = get_threshold(y_test.to_numpy(), deepsad_soft, 0.9700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonn_threshold_09990 = get_threshold(an_df['Predicted Label'].to_numpy(), autonn_soft, 0.9990)\n",
    "autonn_threshold_09950 = get_threshold(an_df['Predicted Label'].to_numpy(), autonn_soft, 0.9950)\n",
    "autonn_threshold_09900 = get_threshold(an_df['Predicted Label'].to_numpy(), autonn_soft, 0.9900)\n",
    "autonn_threshold_09800 = get_threshold(an_df['Predicted Label'].to_numpy(), autonn_soft, 0.9800)\n",
    "autonn_threshold_09700 = get_threshold(an_df['Predicted Label'].to_numpy(), autonn_soft, 0.9700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_threshold_09990 = get_threshold(tabnet_model.y_test.to_numpy(), tabnet_soft, 0.9990)\n",
    "tabnet_threshold_09950 = get_threshold(tabnet_model.y_test.to_numpy(), tabnet_soft, 0.9950)\n",
    "tabnet_threshold_09900 = get_threshold(tabnet_model.y_test.to_numpy(), tabnet_soft, 0.9900)\n",
    "tabnet_threshold_09800 = get_threshold(tabnet_model.y_test.to_numpy(), tabnet_soft, 0.9800)\n",
    "tabnet_threshold_09700 = get_threshold(tabnet_model.y_test.to_numpy(), tabnet_soft, 0.9700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(y_true, y_pred, threshold):\n",
    "    matrix = confusion_matrix(y_true, y_pred > threshold)\n",
    "    return matrix[1][1] / (matrix[0][1] + matrix[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsad_hard = (deepsad_soft > deepsad_threshold_09900).astype(int)\n",
    "autonn_hard = (autonn_soft > autonn_threshold_09900).astype(int)\n",
    "tabnet_hard = (tabnet_soft > tabnet_threshold_09900).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsad_hard = (deepsad_soft > deepsad_threshold_09990).astype(int)\n",
    "autonn_hard = (autonn_soft > autonn_threshold_09990).astype(int)\n",
    "tabnet_hard = (tabnet_soft > tabnet_threshold_09990).astype(int)\n",
    "or_pred_09990 = (np.stack([deepsad_hard, autonn_hard, tabnet_hard]).sum(axis = 0) != 0).astype(int)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), or_pred_09990, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'}, values_format = '.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsad_hard = (deepsad_soft > deepsad_threshold_09950).astype(int)\n",
    "autonn_hard = (autonn_soft > autonn_threshold_09950).astype(int)\n",
    "tabnet_hard = (tabnet_soft > tabnet_threshold_09950).astype(int)\n",
    "or_pred_09950 = (np.stack([deepsad_hard, autonn_hard, tabnet_hard]).sum(axis = 0) != 0).astype(int)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), or_pred_09950, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'}, values_format = '.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsad_hard = (deepsad_soft > deepsad_threshold_09900).astype(int)\n",
    "autonn_hard = (autonn_soft > autonn_threshold_09900).astype(int)\n",
    "tabnet_hard = (tabnet_soft > tabnet_threshold_09900).astype(int)\n",
    "or_pred_09900 = (np.stack([deepsad_hard, autonn_hard, tabnet_hard]).sum(axis = 0) != 0).astype(int)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), or_pred_09900, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'}, values_format = '.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsad_hard = (deepsad_soft > deepsad_threshold_09800).astype(int)\n",
    "autonn_hard = (autonn_soft > autonn_threshold_09800).astype(int)\n",
    "tabnet_hard = (tabnet_soft > tabnet_threshold_09800).astype(int)\n",
    "or_pred_09800 = (np.stack([deepsad_hard, autonn_hard, tabnet_hard]).sum(axis = 0) != 0).astype(int)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), or_pred_09800, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'}, values_format = '.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsad_hard = (deepsad_soft > deepsad_threshold_09700).astype(int)\n",
    "autonn_hard = (autonn_soft > autonn_threshold_09700).astype(int)\n",
    "tabnet_hard = (tabnet_soft > tabnet_threshold_09700).astype(int)\n",
    "or_pred_09700 = (np.stack([deepsad_hard, autonn_hard, tabnet_hard]).sum(axis = 0) != 0).astype(int)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test.to_numpy(), or_pred_09700, normalize = 'true', cmap = plt.cm.Blues, text_kw = {'fontsize': 'x-large'}, values_format = '.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepsad_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.999, 0.90, 100)\n",
    "deepsad_ratio = []\n",
    "autonn_ratio = []\n",
    "tabnet_ratio = []\n",
    "majority_ratio = []\n",
    "or_ratio = []\n",
    "soft_ratio = []\n",
    "\n",
    "for threshold in tqdm(x):\n",
    "    deepsad_threshold = get_threshold(y_test.to_numpy(), deepsad_soft, threshold)\n",
    "    autonn_threshold = get_threshold(an_df['Predicted Label'].to_numpy(), autonn_soft, threshold)\n",
    "    tabnet_threshold = get_threshold(tabnet_model.y_test.to_numpy(), tabnet_soft, threshold)\n",
    "\n",
    "    deepsad_hard = (deepsad_soft > deepsad_threshold).astype(int)\n",
    "    autonn_hard = (autonn_soft > autonn_threshold).astype(int)\n",
    "    tabnet_hard = (tabnet_soft > tabnet_threshold).astype(int)\n",
    "\n",
    "    majority_pred = (np.stack([deepsad_hard, autonn_hard, tabnet_hard]).sum(axis = 0) > 1.5).astype(int)\n",
    "    or_pred = (np.stack([deepsad_hard, autonn_hard, tabnet_hard]).sum(axis = 0) != 0).astype(int)\n",
    "    soft_pred = (np.stack([deepsad_soft, autonn_soft, tabnet_soft]).sum(axis = 0) > deepsad_threshold + autonn_threshold + tabnet_threshold).astype(int)\n",
    "\n",
    "    matrix = confusion_matrix(y_test.to_numpy(), deepsad_hard)\n",
    "    deepsad_ratio.append(matrix[0][1] / (matrix[0][0] + matrix[0][1]))\n",
    "\n",
    "    matrix = confusion_matrix(y_test.to_numpy(), autonn_hard)\n",
    "    autonn_ratio.append(matrix[0][1] / (matrix[0][0] + matrix[0][1]))\n",
    "\n",
    "    matrix = confusion_matrix(y_test.to_numpy(), tabnet_hard)\n",
    "    tabnet_ratio.append(matrix[0][1] / (matrix[0][0] + matrix[0][1]))\n",
    "\n",
    "    matrix = confusion_matrix(y_test.to_numpy(), majority_pred)\n",
    "    majority_ratio.append(matrix[0][1] / (matrix[0][0] + matrix[0][1]))\n",
    "\n",
    "    matrix = confusion_matrix(y_test.to_numpy(), or_pred)\n",
    "    or_ratio.append(matrix[0][1] / (matrix[0][0] + matrix[0][1]))\n",
    "\n",
    "    matrix = confusion_matrix(y_test.to_numpy(), soft_pred)\n",
    "    soft_ratio.append(matrix[0][1] / (matrix[0][0] + matrix[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.linspace(0.999, 0.90, 100), deepsad_ratio, label = 'DeepSAD')\n",
    "# plt.plot(np.linspace(0.999, 0.90, 100), autonn_ratio, label = 'Autoencoder + MLP')\n",
    "# plt.plot(np.linspace(0.999, 0.90, 100), tabnet_ratio, label = 'TabNet')\n",
    "plt.plot(np.linspace(0.999, 0.90, 100), majority_ratio, label = 'Majority Vote')\n",
    "plt.plot(np.linspace(0.999, 0.90, 100), or_ratio, label = 'OR Vote')\n",
    "plt.plot(np.linspace(0.999, 0.90, 100), soft_ratio, label = 'Soft Vote')\n",
    "\n",
    "plt.xlabel('TPR')\n",
    "plt.ylabel('FPR')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibit_ratio = soft_ratio\n",
    "danger_ratio = [x - y for x, y in zip(majority_ratio, soft_ratio)]\n",
    "caution_ratio = [x - y for x, y in zip(or_ratio, majority_ratio)]\n",
    "normal_ratio = [1 - x for x in or_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = [n * 1 + c * 3 + d * 5 + p * 10 for n, c, d, p in zip(normal_ratio, caution_ratio, danger_ratio, prohibit_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0.999, 0.90, 100), elapsed_time, label = 'elapsed time (s)')\n",
    "\n",
    "plt.xlabel('TPR')\n",
    "plt.ylabel('expected elapsed time (s)')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.linspace(0.999, 0.90, 100) == 0.9990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prohibit_ratio[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongjae_rba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd79beb0aae4abe819c785a6067139c3e0331b942f5d097b7bed8a75a3313abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
