{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from deepod.models.tabular import DevNet, PReNet, DeepSAD, FeaWAD, RoSAS\n",
    "# from deepod.metrics import tabular_metrics\n",
    "from autoencodernn import *\n",
    "# from tapnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for filename in tqdm(os.listdir('./data/')):\n",
    "#     if 'preprocessed' in filename:\n",
    "#         dfs.append(pd.read_csv(f'./data/{filename}', index_col = 0))\n",
    "# df = pd.concat(dfs).reset_index(drop = True)\n",
    "# del(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_valid_df, test_df = train_test_split(df, test_size = 0.2, stratify = df['label'])\n",
    "# train_df, valid_df = train_test_split(train_valid_df, test_size = 0.125, stratify = train_valid_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('./checkpoint/train_df.csv')\n",
    "# valid_df.to_csv('./checkpoint/valid_df.csv')\n",
    "# test_df.to_csv('./checkpoint/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_df.drop(['label'], axis = 1)\n",
    "# y_train = train_df['label']\n",
    "\n",
    "# X_valid = valid_df.drop(['label'], axis = 1)\n",
    "# y_valid = valid_df['label']\n",
    "\n",
    "# X_test = test_df.drop(['label'], axis = 1)\n",
    "# y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv('./checkpoint/x_train.csv')\n",
    "# y_train.to_csv('./checkpoint/y_train.csv')\n",
    "\n",
    "# X_valid.to_csv('./checkpoint/x_valid.csv')\n",
    "# y_valid.to_csv('./checkpoint/y_valid.csv')\n",
    "\n",
    "# X_test.to_csv('./checkpoint/x_test.csv')\n",
    "# y_test.to_csv('./checkpoint/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./checkpoint/anomaly_models.pkl', 'rb') as f:\n",
    "#     models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_ad, valid_df_ad, test_df_ad = anomaly_preprocessing(train_df, valid_df, test_df)\n",
    "\n",
    "# X_train_ad = train_df_ad.drop(['label'], axis = 1)\n",
    "# y_train_ad = train_df_ad['label']\n",
    "\n",
    "# X_valid_ad = valid_df_ad.drop(['label'], axis = 1)\n",
    "# y_valid_ad = valid_df_ad['label']\n",
    "\n",
    "# X_test_ad = test_df_ad.drop(['label'], axis = 1)\n",
    "# y_test_ad = test_df_ad['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./checkpoint/train_df.csv', index_col = 0)\n",
    "valid_df = pd.read_csv('./checkpoint/valid_df.csv', index_col = 0)\n",
    "test_df = pd.read_csv('./checkpoint/test_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./checkpoint/x_train.csv', index_col = 0)\n",
    "y_train = pd.read_csv('./checkpoint/y_train.csv', index_col = 0)\n",
    "\n",
    "X_valid = pd.read_csv('./checkpoint/x_valid.csv', index_col = 0)\n",
    "y_valid = pd.read_csv('./checkpoint/y_valid.csv', index_col = 0)\n",
    "\n",
    "X_test = pd.read_csv('./checkpoint/x_test.csv', index_col = 0)\n",
    "y_test = pd.read_csv('./checkpoint/y_test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def anomaly_preprocessing(df1, df2, df3):\n",
    "#     df1['type'] = 'train'\n",
    "#     df2['type'] = 'valid'\n",
    "#     df3['type'] = 'test'\n",
    "\n",
    "#     df = pd.concat([df1, df2, df3]).reset_index(drop = True)\n",
    "\n",
    "#     country_onehot = pd.get_dummies(df['country']).astype(int)\n",
    "#     risk_grades = df[['region_risk_grade', 'city_risk_grade', 'name_risk_grade']]\n",
    "#     browser_onehot = pd.get_dummies(df['browser_name']).astype(int)\n",
    "#     os_onehot = pd.get_dummies(df['os_name']).astype(int)\n",
    "#     legacys = df[['browser_is_legacy', 'os_is_legacy']]\n",
    "#     device_types = pd.get_dummies(df['device_type']).astype(int)\n",
    "#     rtts = df['rtt']\n",
    "#     type = df['type']\n",
    "#     label = df['label']\n",
    "#     df = pd.concat([country_onehot, risk_grades, browser_onehot, os_onehot, legacys, device_types, rtts, type, label], axis = 1)\n",
    "\n",
    "#     df1 = df[df['type'] == 'train'].drop('type', axis = 1)\n",
    "#     df2 = df[df['type'] == 'valid'].drop('type', axis = 1)\n",
    "#     df3 = df[df['type'] == 'test'].drop('type', axis = 1)\n",
    "\n",
    "#     return df1, df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Anomaly Detection\n",
    "# print('Anomaly Detection Model')\n",
    "# ad_model_names = ['DevNet', 'PReNet', 'DeepSAD', 'FeaWAD', 'RoSAS']\n",
    "# ad_models = [\n",
    "#     DevNet(),           # 86.5s\n",
    "#     # PReNet(),           # Too long. (2.4 hours)\n",
    "#     DeepSAD(),          # 75.4s\n",
    "#     FeaWAD(epochs = 10000, lr = 0.01),           # Very fast but poor.\n",
    "#     # RoSAS(),            # Too long.\n",
    "# ]\n",
    "\n",
    "# train_df_ad, valid_df_ad, test_df_ad = anomaly_preprocessing(train_df, valid_df, test_df)\n",
    "\n",
    "# X_train_ad = train_df_ad.drop(['label'], axis = 1)\n",
    "# y_train_ad = train_df_ad['label']\n",
    "\n",
    "# X_valid_ad = valid_df_ad.drop(['label'], axis = 1)\n",
    "# y_valid_ad = valid_df_ad['label']\n",
    "\n",
    "# X_test_ad = test_df_ad.drop(['label'], axis = 1)\n",
    "# y_test_ad = test_df_ad['label']\n",
    "\n",
    "# for model_name, model in zip(ad_model_names, ad_models):\n",
    "#     print('start -', datetime.now())\n",
    "    \n",
    "#     model.fit(X_train_ad.to_numpy(), y_train_ad.to_numpy())\n",
    "#     print('Train Finish')\n",
    "#     pred_train = (model.decision_function(X_train_ad.to_numpy()) > 0.5).astype(int)\n",
    "#     auc_train, ap_train, f1_train = tabular_metrics(y_train_ad, pred_train)\n",
    "    \n",
    "#     pred_valid = (model.decision_function(X_valid_ad.to_numpy()) > 0.5).astype(int)\n",
    "#     auc_valid, ap_valid, f1_valid = tabular_metrics(y_valid_ad, pred_valid)\n",
    "    \n",
    "#     print(f'Trained with {model}')\n",
    "#     print(f'Train - AUC: {auc_train}, AP: {ap_train}, F1: {f1_train}')\n",
    "#     print(f'Valid - AUC: {auc_valid}, AP: {ap_valid}, F1: {f1_valid}')\n",
    "\n",
    "#     models.append(model)\n",
    "#     print('end -', datetime.now(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder + NN Model\n",
      "Epoch 1/50, Loss: 40.46371462152049\n",
      "Epoch 2/50, Loss: 0.1867115788671684\n",
      "Epoch 3/50, Loss: 0.16553188710543482\n",
      "Epoch 4/50, Loss: 0.129700436205546\n",
      "Epoch 5/50, Loss: 0.11284414737053068\n",
      "Epoch 6/50, Loss: 0.10768495442270855\n",
      "Epoch 7/50, Loss: 0.09000545054994767\n",
      "Epoch 8/50, Loss: 0.0803640146048804\n",
      "Epoch 9/50, Loss: 0.07782745462992123\n",
      "Epoch 10/50, Loss: 0.07857066070498357\n",
      "Epoch 11/50, Loss: 0.06472841629371129\n",
      "Epoch 12/50, Loss: 0.06232010015615772\n",
      "Epoch 13/50, Loss: 0.055649134729299624\n",
      "Epoch 14/50, Loss: 0.05286715863697467\n",
      "Epoch 15/50, Loss: 0.057419517312428184\n",
      "Epoch 16/50, Loss: 0.05424163828089153\n",
      "Epoch 17/50, Loss: 0.04791518728768156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# AutoEncoder + NN\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAutoEncoder + NN Model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m autoencoder_nn_model \u001b[39m=\u001b[39m model_v2(\n\u001b[1;32m      4\u001b[0m     train_data \u001b[39m=\u001b[39;49m train_df,          \u001b[39m# Train set\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m     valid_data \u001b[39m=\u001b[39;49m valid_df,          \u001b[39m# Validation set\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m     test_data \u001b[39m=\u001b[39;49m test_df,            \u001b[39m# Test set\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m     criteria \u001b[39m=\u001b[39;49m \u001b[39m0.5\u001b[39;49m,                 \u001b[39m# Classification threshold\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m     split_ratio \u001b[39m=\u001b[39;49m [\u001b[39m7\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m],        \u001b[39m# split ratio (format: [train,validation,test])\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m     autoencoder_epochs \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m,        \u001b[39m# epochs of autoencoder\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m     classifier_epochs \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m,        \u001b[39m# epochs of classifier\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m     weight_for_attack \u001b[39m=\u001b[39;49m \u001b[39m15\u001b[39;49m,         \u001b[39m# weight for attack\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m models\u001b[39m.\u001b[39mappend(autoencoder_nn_model)\n",
      "File \u001b[0;32m~/jupyter/dongjae/2023-fall/RBALogin/autoencodernn.py:80\u001b[0m, in \u001b[0;36mmodel_v2.__init__\u001b[0;34m(self, train_data, valid_data, test_data, criteria, split_ratio, autoencoder_epochs, classifier_epochs, weight_for_attack)\u001b[0m\n\u001b[1;32m     77\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(autoencoder\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m     78\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m---> 80\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_autoencoder(autoencoder, train_iter, criterion, optimizer, epochs\u001b[39m=\u001b[39;49mautoencoder_epochs)\n\u001b[1;32m     82\u001b[0m \u001b[39m# 잠재 벡터 추출\u001b[39;00m\n\u001b[1;32m     83\u001b[0m train_latent_vectors, train_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_latent_vectors(autoencoder, train_iter)\n",
      "File \u001b[0;32m~/jupyter/dongjae/2023-fall/RBALogin/autoencodernn.py:213\u001b[0m, in \u001b[0;36mmodel_v2.train_autoencoder\u001b[0;34m(self, model, data_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m# Autoencoder의 출력과 임베딩된 원본 데이터를 비교\u001b[39;00m\n\u001b[1;32m    212\u001b[0m loss \u001b[39m=\u001b[39m criterion(reconstructed, original_data)\n\u001b[0;32m--> 213\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    214\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    215\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# AutoEncoder + NN\n",
    "print('AutoEncoder + NN Model')\n",
    "autoencoder_nn_model = model_v2(\n",
    "    train_data = train_df,          # Train set\n",
    "    valid_data = valid_df,          # Validation set\n",
    "    test_data = test_df,            # Test set\n",
    "    criteria = 0.5,                 # Classification threshold\n",
    "    split_ratio = [7, 1, 2],        # split ratio (format: [train,validation,test])\n",
    "    autoencoder_epochs = 50,        # epochs of autoencoder\n",
    "    classifier_epochs = 200,        # epochs of classifier\n",
    "    weight_for_attack = 15,         # weight for attack\n",
    ")\n",
    "\n",
    "models.append(autoencoder_nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TabNet\n",
    "# print('TabNet Model')\n",
    "# selected_columns = ['country_code', 'region', 'city_risk_grade', 'name_risk_grade', 'login_success', 'browser_is_legacy', 'os_is_legacy', 'rtt', 'device_type', 'label']\n",
    "# categorical_columns = ['country_code', 'device_type', 'region']\n",
    "\n",
    "# tabnet_model = TabNetModel(train_df, valid_df, test_df, selected_columns, categorical_columns, 'label')         #, pre_train_epochs = 5, epochs = 5)\n",
    "# models.append(tabnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./checkpoint/autoencoder_nn.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongjae_rba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18 (default, Sep 11 2023, 13:40:15) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd79beb0aae4abe819c785a6067139c3e0331b942f5d097b7bed8a75a3313abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
