{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from deepod.models.tabular import DevNet, PReNet, DeepSAD, FeaWAD, RoSAS\n",
    "# from deepod.metrics import tabular_metrics\n",
    "# from autoencodernn import *\n",
    "from tapnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for filename in tqdm(os.listdir('./data/')):\n",
    "#     if 'preprocessed' in filename:\n",
    "#         dfs.append(pd.read_csv(f'./data/{filename}', index_col = 0))\n",
    "# df = pd.concat(dfs).reset_index(drop = True)\n",
    "# del(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_valid_df, test_df = train_test_split(df, test_size = 0.2, stratify = df['label'])\n",
    "# train_df, valid_df = train_test_split(train_valid_df, test_size = 0.125, stratify = train_valid_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('./checkpoint/train_df.csv')\n",
    "# valid_df.to_csv('./checkpoint/valid_df.csv')\n",
    "# test_df.to_csv('./checkpoint/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_df.drop(['label'], axis = 1)\n",
    "# y_train = train_df['label']\n",
    "\n",
    "# X_valid = valid_df.drop(['label'], axis = 1)\n",
    "# y_valid = valid_df['label']\n",
    "\n",
    "# X_test = test_df.drop(['label'], axis = 1)\n",
    "# y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv('./checkpoint/x_train.csv')\n",
    "# y_train.to_csv('./checkpoint/y_train.csv')\n",
    "\n",
    "# X_valid.to_csv('./checkpoint/x_valid.csv')\n",
    "# y_valid.to_csv('./checkpoint/y_valid.csv')\n",
    "\n",
    "# X_test.to_csv('./checkpoint/x_test.csv')\n",
    "# y_test.to_csv('./checkpoint/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./checkpoint/anomaly_models.pkl', 'rb') as f:\n",
    "#     models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_ad, valid_df_ad, test_df_ad = anomaly_preprocessing(train_df, valid_df, test_df)\n",
    "\n",
    "# X_train_ad = train_df_ad.drop(['label'], axis = 1)\n",
    "# y_train_ad = train_df_ad['label']\n",
    "\n",
    "# X_valid_ad = valid_df_ad.drop(['label'], axis = 1)\n",
    "# y_valid_ad = valid_df_ad['label']\n",
    "\n",
    "# X_test_ad = test_df_ad.drop(['label'], axis = 1)\n",
    "# y_test_ad = test_df_ad['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./checkpoint/train_df.csv', index_col = 0)\n",
    "valid_df = pd.read_csv('./checkpoint/valid_df.csv', index_col = 0)\n",
    "test_df = pd.read_csv('./checkpoint/test_df.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./checkpoint/x_train.csv', index_col = 0)\n",
    "y_train = pd.read_csv('./checkpoint/y_train.csv', index_col = 0)\n",
    "\n",
    "X_valid = pd.read_csv('./checkpoint/x_valid.csv', index_col = 0)\n",
    "y_valid = pd.read_csv('./checkpoint/y_valid.csv', index_col = 0)\n",
    "\n",
    "X_test = pd.read_csv('./checkpoint/x_test.csv', index_col = 0)\n",
    "y_test = pd.read_csv('./checkpoint/y_test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def anomaly_preprocessing(df1, df2, df3):\n",
    "#     df1['type'] = 'train'\n",
    "#     df2['type'] = 'valid'\n",
    "#     df3['type'] = 'test'\n",
    "\n",
    "#     df = pd.concat([df1, df2, df3]).reset_index(drop = True)\n",
    "\n",
    "#     country_onehot = pd.get_dummies(df['country']).astype(int)\n",
    "#     risk_grades = df[['region_risk_grade', 'city_risk_grade', 'name_risk_grade']]\n",
    "#     browser_onehot = pd.get_dummies(df['browser_name']).astype(int)\n",
    "#     os_onehot = pd.get_dummies(df['os_name']).astype(int)\n",
    "#     legacys = df[['browser_is_legacy', 'os_is_legacy']]\n",
    "#     device_types = pd.get_dummies(df['device_type']).astype(int)\n",
    "#     rtts = df['rtt']\n",
    "#     type = df['type']\n",
    "#     label = df['label']\n",
    "#     df = pd.concat([country_onehot, risk_grades, browser_onehot, os_onehot, legacys, device_types, rtts, type, label], axis = 1)\n",
    "\n",
    "#     df1 = df[df['type'] == 'train'].drop('type', axis = 1)\n",
    "#     df2 = df[df['type'] == 'valid'].drop('type', axis = 1)\n",
    "#     df3 = df[df['type'] == 'test'].drop('type', axis = 1)\n",
    "\n",
    "#     return df1, df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Anomaly Detection\n",
    "# print('Anomaly Detection Model')\n",
    "# ad_model_names = ['DevNet', 'PReNet', 'DeepSAD', 'FeaWAD', 'RoSAS']\n",
    "# ad_models = [\n",
    "#     DevNet(),           # 86.5s\n",
    "#     # PReNet(),           # Too long. (2.4 hours)\n",
    "#     DeepSAD(),          # 75.4s\n",
    "#     FeaWAD(epochs = 10000, lr = 0.01),           # Very fast but poor.\n",
    "#     # RoSAS(),            # Too long.\n",
    "# ]\n",
    "\n",
    "# train_df_ad, valid_df_ad, test_df_ad = anomaly_preprocessing(train_df, valid_df, test_df)\n",
    "\n",
    "# X_train_ad = train_df_ad.drop(['label'], axis = 1)\n",
    "# y_train_ad = train_df_ad['label']\n",
    "\n",
    "# X_valid_ad = valid_df_ad.drop(['label'], axis = 1)\n",
    "# y_valid_ad = valid_df_ad['label']\n",
    "\n",
    "# X_test_ad = test_df_ad.drop(['label'], axis = 1)\n",
    "# y_test_ad = test_df_ad['label']\n",
    "\n",
    "# for model_name, model in zip(ad_model_names, ad_models):\n",
    "#     print('start -', datetime.now())\n",
    "    \n",
    "#     model.fit(X_train_ad.to_numpy(), y_train_ad.to_numpy())\n",
    "#     print('Train Finish')\n",
    "#     pred_train = (model.decision_function(X_train_ad.to_numpy()) > 0.5).astype(int)\n",
    "#     auc_train, ap_train, f1_train = tabular_metrics(y_train_ad, pred_train)\n",
    "    \n",
    "#     pred_valid = (model.decision_function(X_valid_ad.to_numpy()) > 0.5).astype(int)\n",
    "#     auc_valid, ap_valid, f1_valid = tabular_metrics(y_valid_ad, pred_valid)\n",
    "    \n",
    "#     print(f'Trained with {model}')\n",
    "#     print(f'Train - AUC: {auc_train}, AP: {ap_train}, F1: {f1_train}')\n",
    "#     print(f'Valid - AUC: {auc_valid}, AP: {ap_valid}, F1: {f1_valid}')\n",
    "\n",
    "#     models.append(model)\n",
    "#     print('end -', datetime.now(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AutoEncoder + NN\n",
    "# print('AutoEncoder + NN Model')\n",
    "# autoencoder_nn_model = model_v2(\n",
    "#     train_data = train_df,          # Train set\n",
    "#     valid_data = valid_df,          # Validation set\n",
    "#     test_data = test_df,            # Test set\n",
    "#     criteria = 0.5,                 # Classification threshold\n",
    "#     split_ratio = [7, 1, 2],        # split ratio (format: [train,validation,test])\n",
    "#     autoencoder_epochs = 50,        # epochs of autoencoder\n",
    "#     classifier_epochs = 200,        # epochs of classifier\n",
    "#     weight_for_attack = 15,         # weight for attack\n",
    "# )\n",
    "\n",
    "# models.append(autoencoder_nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[col] = le.fit_transform(train_df[col])\n",
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[col] = le.fit_transform(valid_df[col])\n",
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[col] = le.fit_transform(test_df[col])\n",
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[col] = le.fit_transform(train_df[col])\n",
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[col] = le.fit_transform(valid_df[col])\n",
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[col] = le.fit_transform(test_df[col])\n",
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[col] = le.fit_transform(train_df[col])\n",
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[col] = le.fit_transform(valid_df[col])\n",
      "/home/work/jupyter/dongjae/2023-fall/RBALogin/tapnet.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[col] = le.fit_transform(test_df[col])\n",
      "/home/work/anaconda3/envs/dongjae_rba/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.12379 | val_0_unsup_loss_numpy: 0.7931900024414062|  0:02:34s\n",
      "epoch 1  | loss: 0.72571 | val_0_unsup_loss_numpy: 0.8015900254249573|  0:05:08s\n",
      "epoch 2  | loss: 0.6772  | val_0_unsup_loss_numpy: 0.6742100119590759|  0:07:56s\n",
      "epoch 3  | loss: 0.64166 | val_0_unsup_loss_numpy: 0.7504799962043762|  0:10:49s\n",
      "epoch 4  | loss: 0.62238 | val_0_unsup_loss_numpy: 0.7077000141143799|  0:13:49s\n",
      "epoch 5  | loss: 0.61059 | val_0_unsup_loss_numpy: 0.7312800288200378|  0:17:04s\n",
      "epoch 6  | loss: 0.61275 | val_0_unsup_loss_numpy: 0.6628999710083008|  0:20:35s\n",
      "epoch 7  | loss: 0.60581 | val_0_unsup_loss_numpy: 0.6411299705505371|  0:23:41s\n",
      "epoch 8  | loss: 0.60063 | val_0_unsup_loss_numpy: 0.6268600225448608|  0:27:08s\n",
      "epoch 9  | loss: 0.60043 | val_0_unsup_loss_numpy: 0.5914700031280518|  0:30:45s\n",
      "epoch 10 | loss: 0.59469 | val_0_unsup_loss_numpy: 0.5770800113677979|  0:33:55s\n",
      "epoch 11 | loss: 0.59025 | val_0_unsup_loss_numpy: 0.5900099873542786|  0:37:13s\n",
      "epoch 12 | loss: 0.59068 | val_0_unsup_loss_numpy: 0.5859599709510803|  0:40:16s\n",
      "epoch 13 | loss: 0.58837 | val_0_unsup_loss_numpy: 0.55867999792099|  0:43:22s\n",
      "epoch 14 | loss: 0.58783 | val_0_unsup_loss_numpy: 0.5727800130844116|  0:46:40s\n",
      "epoch 15 | loss: 0.58856 | val_0_unsup_loss_numpy: 0.5803800225257874|  0:49:37s\n",
      "epoch 16 | loss: 0.58646 | val_0_unsup_loss_numpy: 0.5588399767875671|  0:52:55s\n",
      "epoch 17 | loss: 0.58615 | val_0_unsup_loss_numpy: 0.5468199849128723|  0:56:08s\n",
      "epoch 18 | loss: 0.58534 | val_0_unsup_loss_numpy: 0.5476800203323364|  0:59:11s\n",
      "epoch 19 | loss: 0.58062 | val_0_unsup_loss_numpy: 0.5624499917030334|  1:02:28s\n",
      "epoch 20 | loss: 0.58057 | val_0_unsup_loss_numpy: 0.5432299971580505|  1:05:31s\n",
      "epoch 21 | loss: 0.58094 | val_0_unsup_loss_numpy: 0.5438699722290039|  1:08:42s\n"
     ]
    }
   ],
   "source": [
    "# TabNet\n",
    "print('TabNet Model')\n",
    "selected_columns = ['country_code', 'region', 'city_risk_grade', 'name_risk_grade', 'login_success', 'browser_is_legacy', 'os_is_legacy', 'rtt', 'device_type', 'label']\n",
    "categorical_columns = ['country_code', 'device_type', 'region']\n",
    "\n",
    "tabnet_model = TabNetModel(train_df, valid_df, test_df, selected_columns, categorical_columns, 'label')         #, pre_train_epochs = 5, epochs = 5)\n",
    "models.append(tabnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./checkpoint/tabnet.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongjae_rba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18 (default, Sep 11 2023, 13:40:15) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd79beb0aae4abe819c785a6067139c3e0331b942f5d097b7bed8a75a3313abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
